"""
Webhook æ¥­å‹™é‚è¼¯æœå‹™
è™•ç† Amazon ç”¢å“æŠ“å– Webhook çš„è¤‡é›œæ¥­å‹™é‚è¼¯
"""

import logging
from datetime import date, datetime
from typing import Any, Dict

from shared.collectors.amazon_data_collector import AmazonDataCollector
from shared.database.asin_status_queries import bulk_update_asin_status
from shared.database.model_types import ProductSnapshotDict
from shared.database.products_queries import bulk_update_products
from shared.database.snapshots_queries import bulk_create_snapshots

from .alert_check_service import AlertCheckService

# è¨­å®š logger
logger = logging.getLogger(__name__)


class WebhookService:
    """Webhook æ¥­å‹™é‚è¼¯æœå‹™é¡"""

    def __init__(self, alert_check_service: AlertCheckService = None):
        """
        åˆå§‹åŒ–æœå‹™

        Args:
            alert_check_service (AlertCheckService, optional): å‘Šè­¦æª¢æŸ¥æœå‹™å¯¦ä¾‹
        """
        # åˆå§‹åŒ– Amazon è³‡æ–™æ”¶é›†å™¨
        try:
            self.amazon_collector = AmazonDataCollector()
            logger.info("âœ… Amazon è³‡æ–™æ”¶é›†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Amazon è³‡æ–™æ”¶é›†å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            self.amazon_collector = None

        # åˆå§‹åŒ–å‘Šè­¦æª¢æŸ¥æœå‹™
        self.alert_check_service = alert_check_service
        if self.alert_check_service:
            logger.info("âœ… å‘Šè­¦æª¢æŸ¥æœå‹™å·²åˆå§‹åŒ–")
        else:
            logger.warning("âš ï¸ å‘Šè­¦æª¢æŸ¥æœå‹™æœªåˆå§‹åŒ–")

    async def process_amazon_webhook(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è™•ç† Amazon ç”¢å“æŠ“å– Webhook

        Args:
            data: Webhook è³‡æ–™

        Returns:
            è™•ç†çµæœ
        """
        # åˆå§‹åŒ–éŸ¿æ‡‰è³‡æ–™
        response_data = {
            "status": "success",
            "message": "Webhook æ¥æ”¶æˆåŠŸ",
            "timestamp": datetime.now().isoformat(),
            "event_type": None,
            "processed": False,
        }

        try:
            # æª¢æŸ¥æ˜¯å¦åŒ…å« Apify ç‰¹å®šçš„è³‡æ–™
            if isinstance(data, dict):
                event_type = data.get("eventType")
                logger.info(f"ğŸ­ Event Type: {event_type}")

                # æ›´æ–°éŸ¿æ‡‰è³‡æ–™ä¸­çš„äº‹ä»¶é¡å‹
                response_data["event_type"] = event_type
                response_data["processed"] = event_type == "ACTOR.RUN.SUCCEEDED"

                if "eventData" in data:
                    event_data = data.get("eventData")
                    logger.info(f"ğŸƒ Actor Run ID: {event_data.get('actorRunId')}")
                    logger.info(f"ğŸ­ Actor ID: {event_data.get('actorId')}")

                if "resource" in data:
                    resource = data.get("resource")
                    logger.info(f"ğŸ“Š ç‹€æ…‹: {resource.get('status')}")
                    logger.info(f"ğŸ’¾ Dataset ID: {resource.get('defaultDatasetId')}")
                    logger.info(f"â° é–‹å§‹æ™‚é–“: {resource.get('startedAt')}")
                    logger.info(f"â° çµæŸæ™‚é–“: {resource.get('finishedAt')}")

                    # è™•ç† ACTOR.RUN.SUCCEEDED äº‹ä»¶
                    if (
                        event_type == "ACTOR.RUN.SUCCEEDED"
                        and resource.get("status") == "SUCCEEDED"
                    ):
                        result = await self._process_successful_webhook(resource)
                        response_data.update(result)

                    # è™•ç†é SUCCEEDED äº‹ä»¶
                    elif (
                        event_type != "ACTOR.RUN.SUCCEEDED"
                        or resource.get("status") != "SUCCEEDED"
                    ):
                        logger.warning(
                            f"âš ï¸ æ”¶åˆ°éæˆåŠŸäº‹ä»¶: {event_type}, ç‹€æ…‹: {resource.get('status')}"
                        )
                        logger.warning(
                            "ğŸ”„ éœ€è¦æ›´æ–°ç›¸é—œ ASIN ç‹€æ…‹ç‚º failedï¼Œä½†ç„¡æ³•å¾ç•¶å‰ webhook è³‡æ–™ä¸­ç²å– ASIN åˆ—è¡¨"
                        )
                        logger.warning(
                            "   å»ºè­°ï¼šåœ¨ Celery ä»»å‹™ä¸­è¨˜éŒ„ run_id èˆ‡ ASIN çš„å°æ‡‰é—œä¿‚ï¼Œä»¥ä¾¿åœ¨æ­¤è™•æŸ¥è©¢"
                        )

            return response_data

        except Exception as e:
            logger.error(f"âŒ Webhook è™•ç†éŒ¯èª¤: {str(e)}")
            response_data.update(
                {"status": "error", "message": f"Webhook è™•ç†å¤±æ•—: {str(e)}"}
            )
            return response_data

    async def _process_successful_webhook(
        self, resource: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        è™•ç†æˆåŠŸçš„ Webhook äº‹ä»¶

        Args:
            resource: è³‡æºè³‡æ–™

        Returns:
            è™•ç†çµæœ
        """
        dataset_id = resource.get("defaultDatasetId")
        if not dataset_id or not self.amazon_collector:
            logger.warning(
                f"âš ï¸ ç„¡æ³•æŠ“å–è³‡æ–™: Dataset ID={dataset_id}, Collector={self.amazon_collector is not None}"
            )
            return {}

        logger.info(f"ğŸ”„ é–‹å§‹å¾ Dataset {dataset_id} æŠ“å–è³‡æ–™...")
        try:
            # ä½¿ç”¨ Amazon è³‡æ–™æ”¶é›†å™¨æŠ“å–è³‡æ–™
            dataset_items = await self.amazon_collector.get_parsed_dataset_items(
                dataset_id
            )
            logger.info(f"âœ… æˆåŠŸæŠ“å– {len(dataset_items)} ç­†ç”¢å“è³‡æ–™")

            if not dataset_items:
                logger.warning("âš ï¸ æ²’æœ‰æŠ“å–åˆ°ä»»ä½•ç”¢å“è³‡æ–™")
                return {}

            # æº–å‚™æ‰¹é‡æ›´æ–° products å’Œå‰µå»º snapshots çš„è³‡æ–™
            products_data = []
            snapshots_data = []
            current_date = date.today()

            for i, item in enumerate(dataset_items, 1):
                logger.info(
                    f"   è™•ç†ç”¢å“ {i}/{len(dataset_items)}: {item.get('asin', 'Unknown')}"
                )

                # æº–å‚™ products è³‡æ–™
                product_data = {
                    "asin": item.get("asin"),
                    "title": item.get("title"),
                    "categories": item.get("categories", []),
                }
                products_data.append(product_data)

                # æº–å‚™ snapshots è³‡æ–™
                snapshot_data = ProductSnapshotDict(
                    asin=item.get("asin"),
                    snapshot_date=current_date,
                    price=item.get("price"),
                    rating=item.get("rating"),
                    review_count=item.get("review_count"),
                    bsr_data=item.get("bsr", []),
                    raw_data=item,  # å„²å­˜å®Œæ•´çš„è§£æå¾Œè³‡æ–™
                )
                snapshots_data.append(snapshot_data)

            # æ‰¹é‡æ›´æ–° products è¡¨
            logger.info(f"ğŸ”„ é–‹å§‹æ‰¹é‡æ›´æ–° {len(products_data)} ç­†ç”¢å“è³‡æ–™...")
            products_success = bulk_update_products(products_data)
            if products_success:
                logger.info(f"âœ… æˆåŠŸæ›´æ–° {len(products_data)} ç­†ç”¢å“è³‡æ–™")
            else:
                logger.error("âŒ æ›´æ–°ç”¢å“è³‡æ–™å¤±æ•—")

            # æ‰¹é‡å‰µå»º snapshots
            logger.info(f"ğŸ”„ é–‹å§‹æ‰¹é‡å‰µå»º {len(snapshots_data)} ç­†å¿«ç…§è³‡æ–™...")
            snapshots_success = bulk_create_snapshots(snapshots_data)
            if snapshots_success:
                logger.info(f"âœ… æˆåŠŸå‰µå»º {len(snapshots_data)} ç­†å¿«ç…§è³‡æ–™")
            else:
                logger.error("âŒ å‰µå»ºå¿«ç…§è³‡æ–™å¤±æ•—")

            # ç¸½çµè™•ç†çµæœä¸¦æ›´æ–° ASIN ç‹€æ…‹
            if products_success and snapshots_success:
                logger.info(f"ğŸ‰ æˆåŠŸè™•ç† {len(dataset_items)} ç­†ç”¢å“è³‡æ–™")

                # ç²å–æ‰€æœ‰è™•ç†æˆåŠŸçš„ ASIN
                successful_asins = [
                    item.get("asin") for item in dataset_items if item.get("asin")
                ]

                # æ›´æ–° ASIN ç‹€æ…‹ç‚º completed
                logger.info(
                    f"ğŸ”„ æ›´æ–° {len(successful_asins)} å€‹ ASIN ç‹€æ…‹ç‚º completed..."
                )
                status_update_result = bulk_update_asin_status(
                    successful_asins, "completed"
                )

                if status_update_result["success"]:
                    logger.info(
                        f"âœ… æˆåŠŸæ›´æ–° {status_update_result['success_count']} å€‹ ASIN ç‹€æ…‹ç‚º completed"
                    )
                    if status_update_result["failed_asins"]:
                        logger.warning(
                            f"âš ï¸ æœ‰ {len(status_update_result['failed_asins'])} å€‹ ASIN ç‹€æ…‹æ›´æ–°å¤±æ•—: {status_update_result['failed_asins']}"
                        )
                else:
                    logger.error(
                        f"âŒ ASIN ç‹€æ…‹æ›´æ–°å¤±æ•—: {status_update_result['message']}"
                    )

                # æª¢æŸ¥å‘Šè­¦ï¼ˆå¦‚æœå‘Šè­¦æª¢æŸ¥æœå‹™å¯ç”¨ï¼‰
                alert_results = {}
                if self.alert_check_service:
                    logger.info(
                        f"ğŸ” é–‹å§‹æª¢æŸ¥ {len(successful_asins)} å€‹ ASIN çš„å‘Šè­¦..."
                    )
                    try:
                        alert_results = (
                            await self.alert_check_service.check_alerts_for_asins(
                                successful_asins
                            )
                        )
                        total_alerts = sum(
                            len(alerts) for alerts in alert_results.values()
                        )
                        logger.info(f"âœ… å‘Šè­¦æª¢æŸ¥å®Œæˆï¼Œç¸½å…±è§¸ç™¼ {total_alerts} å€‹å‘Šè­¦")
                    except Exception as e:
                        logger.error(f"âŒ å‘Šè­¦æª¢æŸ¥å¤±æ•—: {e}")
                        alert_results = {}
                else:
                    logger.warning("âš ï¸ å‘Šè­¦æª¢æŸ¥æœå‹™ä¸å¯ç”¨ï¼Œè·³éå‘Šè­¦æª¢æŸ¥")

                return {
                    "products_updated": len(products_data),
                    "snapshots_created": len(snapshots_data),
                    "total_processed": len(dataset_items),
                    "asin_status_update": status_update_result,
                    "alerts_triggered": alert_results,
                }
            else:
                logger.warning("âš ï¸ éƒ¨åˆ†è³‡æ–™è™•ç†å¤±æ•—")

                # ç²å–æ‰€æœ‰ ASINï¼ˆå³ä½¿è™•ç†å¤±æ•—ä¹Ÿè¦æ›´æ–°ç‹€æ…‹ï¼‰
                all_asins = [
                    item.get("asin") for item in dataset_items if item.get("asin")
                ]

                # æ›´æ–° ASIN ç‹€æ…‹ç‚º failed
                logger.info(f"ğŸ”„ æ›´æ–° {len(all_asins)} å€‹ ASIN ç‹€æ…‹ç‚º failed...")
                status_update_result = bulk_update_asin_status(all_asins, "failed")

                if status_update_result["success"]:
                    logger.info(
                        f"âœ… æˆåŠŸæ›´æ–° {status_update_result['success_count']} å€‹ ASIN ç‹€æ…‹ç‚º failed"
                    )
                else:
                    logger.error(
                        f"âŒ ASIN ç‹€æ…‹æ›´æ–°å¤±æ•—: {status_update_result['message']}"
                    )

                return {
                    "products_updated": len(products_data) if products_success else 0,
                    "snapshots_created": (
                        len(snapshots_data) if snapshots_success else 0
                    ),
                    "total_processed": len(dataset_items),
                    "asin_status_update": status_update_result,
                }

        except Exception as e:
            logger.error(f"âŒ å¾ Dataset æŠ“å–è³‡æ–™å¤±æ•—: {e}")
            return {}
